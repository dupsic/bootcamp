{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3bde35e6",
      "metadata": {
        "id": "3bde35e6"
      },
      "source": [
        "## 1. Setup & Installation\n",
        "\n",
        "First, let's install all required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "579a21a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "579a21a8",
        "outputId": "e8c61619-7781-4d6c-9fb2-9f10cbca3659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# langgraph: The graph-based agent framework\n",
        "# langchain-openai: OpenAI integration for LangChain\n",
        "# langchain: Core LangChain library\n",
        "# langchain-core: Core abstractions for LangChain\n",
        "\n",
        "!pip install -q langgraph langchain-openai langchain langchain-core"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda9f312",
      "metadata": {
        "id": "cda9f312"
      },
      "source": [
        "## 2. API Key Configuration\n",
        "\n",
        "To use OpenAI API, you need an API key from [OpenAI Platform](https://platform.openai.com/api-keys).\n",
        "\n",
        "**Security Best Practice**: Never hardcode API keys in your code. Use environment variables or a secure configuration method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f74eea0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f74eea0e",
        "outputId": "2ee50d8d-8365-4b2e-b205-14b5c25e0ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your OpenAI Base URL: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "if \"OPENAI_BASE_URL\" not in os.environ:\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = getpass.getpass(\"Enter your OpenAI Base URL: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45dd7783",
      "metadata": {
        "id": "45dd7783"
      },
      "source": [
        "## 3. Model Configuration\n",
        "\n",
        "Let's create a reusable model configuration using OpenAI's gpt-4o-mini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a9f21d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a9f21d0",
        "outputId": "d6c1b3d1-d25d-431a-b600-06814dc125e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: Hello! Yes, I'm here and ready to help. How can I assist you today?\n",
            "\n",
            "âœ… Model configured and tested successfully!\n",
            "ðŸ“‹ Base URL: https://ds-ai-internship.openai.azure.com/openai/v1/\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Configure the gpt-4.1-mini model via OpenAI\n",
        "def get_model(temperature=0.7):\n",
        "    \"\"\"\n",
        "    Create and return a configured gpt-4.1-mini model via OpenAI.\n",
        "\n",
        "    Args:\n",
        "        temperature (float): Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
        "\n",
        "    Returns:\n",
        "        ChatOpenAI: Configured model instance\n",
        "    \"\"\"\n",
        "    return ChatOpenAI(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        temperature=temperature,\n",
        "        max_tokens=2048,\n",
        "        base_url=os.environ.get(\"OPENAI_BASE_URL\"),\n",
        "    )\n",
        "\n",
        "# Test the model\n",
        "test_model = get_model()\n",
        "response = test_model.invoke(\"Hello! Can you confirm you're working?\")\n",
        "print(f\"Model Response: {response.content}\")\n",
        "print(f\"\\nâœ… Model configured and tested successfully!\")\n",
        "print(f\"ðŸ“‹ Base URL: {os.environ.get('OPENAI_BASE_URL')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e3233a",
      "metadata": {
        "id": "60e3233a"
      },
      "source": [
        "## 4. Import Required Libraries\n",
        "\n",
        "Let's import all the necessary libraries for building our agentic patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "01d8c6aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01d8c6aa",
        "outputId": "97829897-68a3-4e76-b5f9-b51fb916a536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import operator\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9726bb",
      "metadata": {
        "id": "8d9726bb"
      },
      "source": [
        "---\n",
        "\n",
        "# Agentic Patterns\n",
        "\n",
        "Now let's explore each agentic pattern with conceptual explanations and working code examples.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a241f733",
      "metadata": {
        "id": "a241f733"
      },
      "source": [
        "## Pattern A: Prompt Chaining\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Prompt Chaining** is a sequential workflow pattern where the output of one agent becomes the input of the next agent. This creates a pipeline of transformations.\n",
        "\n",
        "**Use Cases:**\n",
        "- Multi-step content generation (outline â†’ draft â†’ polish)\n",
        "- Data processing pipelines (extract â†’ transform â†’ analyze)\n",
        "- Sequential reasoning tasks\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Input â†’ Agent A â†’ Intermediate Result â†’ Agent B â†’ Final Output\n",
        "```\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a63cbb53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a63cbb53",
        "outputId": "fa706ed0-e56b-4a50-c7a8-20ca8696c909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Prompt Chaining workflow created!\n"
          ]
        }
      ],
      "source": [
        "# Define the state structure for prompt chaining\n",
        "class ChainState(TypedDict):\n",
        "    topic: str\n",
        "    outline: str\n",
        "    article: str\n",
        "\n",
        "# Agent A: Creates an outline\n",
        "def create_outline(state: ChainState) -> ChainState:\n",
        "    \"\"\"\n",
        "    Agent A: Generates a structured outline for the given topic.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”µ Agent A: Creating outline...\")\n",
        "\n",
        "    model = get_model(temperature=0.7)\n",
        "    prompt = f\"Create a brief 3-point outline for an article about: {state['topic']}\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"outline\"] = response.content\n",
        "\n",
        "    print(f\"âœ… Outline created:\\n{state['outline']}\\n\")\n",
        "    return state\n",
        "\n",
        "# Agent B: Expands the outline into a full article\n",
        "def write_article(state: ChainState) -> ChainState:\n",
        "    \"\"\"\n",
        "    Agent B: Takes the outline and writes a complete article.\n",
        "    \"\"\"\n",
        "    print(\"ðŸŸ¢ Agent B: Writing article from outline...\")\n",
        "\n",
        "    model = get_model(temperature=0.8)\n",
        "    prompt = f\"\"\"Based on this outline, write a short article (3-4 paragraphs):\n",
        "\n",
        "Outline:\n",
        "{state['outline']}\n",
        "\n",
        "Article:\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"article\"] = response.content\n",
        "\n",
        "    print(f\"âœ… Article written!\\n\")\n",
        "    return state\n",
        "\n",
        "# Build the graph\n",
        "workflow_chain = StateGraph(ChainState)\n",
        "\n",
        "# Add nodes (agents)\n",
        "workflow_chain.add_node(\"outline_agent\", create_outline)\n",
        "workflow_chain.add_node(\"writer_agent\", write_article)\n",
        "\n",
        "# Define the flow: START â†’ outline_agent â†’ writer_agent â†’ END\n",
        "workflow_chain.add_edge(START, \"outline_agent\")\n",
        "workflow_chain.add_edge(\"outline_agent\", \"writer_agent\")\n",
        "workflow_chain.add_edge(\"writer_agent\", END)\n",
        "\n",
        "# Compile the graph\n",
        "chain_app = workflow_chain.compile()\n",
        "\n",
        "print(\"âœ… Prompt Chaining workflow created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3f98321c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f98321c",
        "outputId": "6af09aa1-7620-425e-a190-043c0b73936f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Testing Prompt Chaining Pattern\n",
            "============================================================\n",
            "ðŸ”µ Agent A: Creating outline...\n",
            "âœ… Outline created:\n",
            "1. Emerging Trends and Technologies in AI  \n",
            "2. Potential Societal and Economic Impacts  \n",
            "3. Ethical Considerations and Regulatory Challenges\n",
            "\n",
            "ðŸŸ¢ Agent B: Writing article from outline...\n",
            "âœ… Article written!\n",
            "\n",
            "\n",
            "============================================================\n",
            "FINAL ARTICLE:\n",
            "============================================================\n",
            "Artificial intelligence (AI) continues to evolve at a rapid pace, with emerging trends and technologies reshaping industries and everyday life. Advances in natural language processing, computer vision, and autonomous systems are enabling machines to perform increasingly complex tasks with greater accuracy and efficiency. Innovations such as generative AI models, reinforcement learning, and edge AI are expanding the possibilities for personalized experiences, real-time decision-making, and smart automation. These technological breakthroughs promise to revolutionize sectors ranging from healthcare and finance to transportation and education, fostering a new era of digital transformation.\n",
            "\n",
            "The societal and economic impacts of these AI advancements are profound and multifaceted. On one hand, AI-driven automation has the potential to increase productivity, reduce costs, and create new job categories that require advanced skills. On the other hand, it also raises concerns about workforce displacement, widening inequality, and the digital divide. Economies that adapt quickly to integrate AI technologies may gain a competitive edge, while others risk falling behind. Furthermore, the widespread adoption of AI could alter how people interact with technology, redefine social norms, and influence global power dynamics.\n",
            "\n",
            "Amid these opportunities and challenges, ethical considerations and regulatory frameworks become critical. Issues such as data privacy, algorithmic bias, transparency, and accountability demand careful attention to ensure AI systems are fair, trustworthy, and respect human rights. Policymakers face the difficult task of balancing innovation with safeguards, crafting regulations that promote responsible AI development without stifling creativity. Collaborative efforts among governments, industry leaders, and civil society will be essential to navigate these complexities and harness AIâ€™s benefits for the greater good. As AI continues to advance, a proactive and inclusive approach to ethics and governance will be key to shaping its positive impact on society.\n"
          ]
        }
      ],
      "source": [
        "# Test the Prompt Chaining pattern\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Prompt Chaining Pattern\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = chain_app.invoke({\n",
        "    \"topic\": \"The Future of Artificial Intelligence\",\n",
        "    \"outline\": \"\",\n",
        "    \"article\": \"\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL ARTICLE:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"article\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e054a4",
      "metadata": {
        "id": "35e054a4"
      },
      "source": [
        "## Pattern B: Routing\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Routing** is a pattern where a router agent analyzes the input and dynamically decides which specialized agent or tool to invoke based on the query type.\n",
        "\n",
        "**Use Cases:**\n",
        "- Customer service chatbots (route to billing, technical, or sales)\n",
        "- Multi-domain question answering\n",
        "- Task classification and delegation\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "                    â”Œâ”€â”€> Specialist Agent A\n",
        "Input â†’ Router Agent â”¼â”€â”€> Specialist Agent B\n",
        "                    â””â”€â”€> Specialist Agent C\n",
        "```\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1607af2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1607af2",
        "outputId": "68b6fc48-7121-4967-aff0-92e0101a8736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Routing workflow created!\n"
          ]
        }
      ],
      "source": [
        "# Define the state structure for routing\n",
        "class RouteState(TypedDict):\n",
        "    query: str\n",
        "    route: str\n",
        "    response: str\n",
        "\n",
        "# Router Agent: Determines which specialist to call\n",
        "def route_query(state: RouteState) -> RouteState:\n",
        "    \"\"\"\n",
        "    Router Agent: Analyzes the query and determines the appropriate route.\n",
        "    Routes: 'technical', 'creative', 'general'\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”€ Router Agent: Analyzing query...\")\n",
        "\n",
        "    model = get_model(temperature=0.3)\n",
        "    prompt = f\"\"\"Classify this query into one category: 'technical', 'creative', or 'general'.\n",
        "Respond with ONLY the category name.\n",
        "\n",
        "Query: {state['query']}\n",
        "\n",
        "Category:\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    route = response.content.strip().lower()\n",
        "\n",
        "    # Validate route\n",
        "    if route not in ['technical', 'creative', 'general']:\n",
        "        route = 'general'\n",
        "\n",
        "    state[\"route\"] = route\n",
        "    print(f\"âœ… Query routed to: {route}\\n\")\n",
        "    return state\n",
        "\n",
        "# Technical Specialist Agent\n",
        "def technical_agent(state: RouteState) -> RouteState:\n",
        "    \"\"\"\n",
        "    Technical Specialist: Handles technical and scientific queries.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”§ Technical Agent: Processing query...\")\n",
        "\n",
        "    model = get_model(temperature=0.5)\n",
        "    prompt = f\"\"\"As a technical expert, provide a precise and detailed answer:\n",
        "\n",
        "{state['query']}\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"response\"] = f\"[Technical Agent] {response.content}\"\n",
        "    return state\n",
        "\n",
        "# Creative Specialist Agent\n",
        "def creative_agent(state: RouteState) -> RouteState:\n",
        "    \"\"\"\n",
        "    Creative Specialist: Handles creative and artistic queries.\n",
        "    \"\"\"\n",
        "    print(\"ðŸŽ¨ Creative Agent: Processing query...\")\n",
        "\n",
        "    model = get_model(temperature=0.9)\n",
        "    prompt = f\"\"\"As a creative writer, provide an imaginative and engaging answer:\n",
        "\n",
        "{state['query']}\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"response\"] = f\"[Creative Agent] {response.content}\"\n",
        "    return state\n",
        "\n",
        "# General Specialist Agent\n",
        "def general_agent(state: RouteState) -> RouteState:\n",
        "    \"\"\"\n",
        "    General Specialist: Handles general queries.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ“ General Agent: Processing query...\")\n",
        "\n",
        "    model = get_model(temperature=0.7)\n",
        "    prompt = f\"\"\"Provide a helpful and balanced answer to this query:\n",
        "\n",
        "{state['query']}\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"response\"] = f\"[General Agent] {response.content}\"\n",
        "    return state\n",
        "\n",
        "# Conditional routing function\n",
        "def route_to_specialist(state: RouteState) -> str:\n",
        "    \"\"\"\n",
        "    Returns the name of the next node based on the route.\n",
        "    \"\"\"\n",
        "    route = state[\"route\"]\n",
        "    if route == \"technical\":\n",
        "        return \"technical_specialist\"\n",
        "    elif route == \"creative\":\n",
        "        return \"creative_specialist\"\n",
        "    else:\n",
        "        return \"general_specialist\"\n",
        "\n",
        "# Build the routing graph\n",
        "workflow_route = StateGraph(RouteState)\n",
        "\n",
        "# Add nodes\n",
        "workflow_route.add_node(\"router\", route_query)\n",
        "workflow_route.add_node(\"technical_specialist\", technical_agent)\n",
        "workflow_route.add_node(\"creative_specialist\", creative_agent)\n",
        "workflow_route.add_node(\"general_specialist\", general_agent)\n",
        "\n",
        "# Define the flow\n",
        "workflow_route.add_edge(START, \"router\")\n",
        "workflow_route.add_conditional_edges(\n",
        "    \"router\",\n",
        "    route_to_specialist,\n",
        "    {\n",
        "        \"technical_specialist\": \"technical_specialist\",\n",
        "        \"creative_specialist\": \"creative_specialist\",\n",
        "        \"general_specialist\": \"general_specialist\"\n",
        "    }\n",
        ")\n",
        "workflow_route.add_edge(\"technical_specialist\", END)\n",
        "workflow_route.add_edge(\"creative_specialist\", END)\n",
        "workflow_route.add_edge(\"general_specialist\", END)\n",
        "\n",
        "# Compile the graph\n",
        "route_app = workflow_route.compile()\n",
        "\n",
        "print(\"âœ… Routing workflow created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4b1a8c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b1a8c96",
        "outputId": "a1e6c845-6b11-41b9-be5f-a2e00ae1e20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Query: Explain how neural networks learn through backpropagation\n",
            "============================================================\n",
            "ðŸ”€ Router Agent: Analyzing query...\n",
            "âœ… Query routed to: technical\n",
            "\n",
            "ðŸ”§ Technical Agent: Processing query...\n",
            "\n",
            "[Technical Agent] Certainly! Here's a precise and detailed explanation of how neural networks learn through **backpropagation**:\n",
            "\n",
            "---\n",
            "\n",
            "### Overview\n",
            "\n",
            "Backpropagation (short for **backward propagation of errors**) is a supervised learning algorithm used to train artificial neural networks. It efficiently computes the gradient of the loss function with respect to each weight in the network, enabling the use of gradient-based optimization methods (like gradient descent) to update the weights and minimize prediction error.\n",
            "\n",
            "---\n",
            "\n",
            "### Components Involved\n",
            "\n",
            "1. **Neural Network Structure:**\n",
            "   - Composed of layers: input layer, one or more hidden layers, and an output layer.\n",
            "   - Each layer consists of neurons (units) with associated weights and biases.\n",
            "   - Each neuron applies a weighted sum of its inputs followed by a nonlinear activation function.\n",
            "\n",
            "2. **Forward Pass:**\n",
            "   - Input data is passed through the network layer-by-layer.\n",
            "   - At each neuron, compute the weighted sum of inputs plus bias and apply an activation function.\n",
            "   - The output layer produces the networkâ€™s prediction.\n",
            "\n",
            "3. **Loss Function:**\n",
            "   - Measures the difference between the predicted output and the true target.\n",
            "   - Common examples include Mean Squared Error (MSE) for regression or Cross-Entropy Loss for classification.\n",
            "\n",
            "---\n",
            "\n",
            "### Backpropagation Algorithm Steps\n",
            "\n",
            "#### 1. Forward Pass\n",
            "\n",
            "- For each training example:\n",
            "  - Compute activations \\( a^{(l)} \\) for each layer \\( l \\) using:\n",
            "    \\[\n",
            "    z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
            "    \\]\n",
            "    \\[\n",
            "    a^{(l)} = \\sigma(z^{(l)})\n",
            "    \\]\n",
            "  - Here, \\( W^{(l)} \\) and \\( b^{(l)} \\) are weights and biases of layer \\( l \\), and \\( \\sigma \\) is the activation function.\n",
            "\n",
            "- Obtain the output \\( a^{(L)} \\) at the final layer \\( L \\).\n",
            "\n",
            "#### 2. Compute Loss\n",
            "\n",
            "- Calculate the loss \\( \\mathcal{L}(a^{(L)}, y) \\) comparing output to the true label \\( y \\).\n",
            "\n",
            "#### 3. Backward Pass (Backward Propagation of Errors)\n",
            "\n",
            "- Goal: Compute gradients of the loss with respect to weights and biases, \\( \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} \\) and \\( \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} \\), for all layers \\( l \\).\n",
            "\n",
            "- Start from the output layer and move backward.\n",
            "\n",
            "##### Step 3.1: Compute error term for output layer\n",
            "\n",
            "\\[\n",
            "\\delta^{(L)} = \\nabla_{a^{(L)}} \\mathcal{L} \\odot \\sigma'(z^{(L)})\n",
            "\\]\n",
            "\n",
            "- \\( \\nabla_{a^{(L)}} \\mathcal{L} \\) is the gradient of the loss w.r.t output activations.\n",
            "- \\( \\sigma'(z^{(L)}) \\) is the derivative of the activation function at layer \\( L \\).\n",
            "- \\( \\odot \\) denotes element-wise multiplication.\n",
            "\n",
            "##### Step 3.2: Propagate error backward through hidden layers\n",
            "\n",
            "For each layer \\( l = L-1, L-2, ..., 1 \\):\n",
            "\n",
            "\\[\n",
            "\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})\n",
            "\\]\n",
            "\n",
            "- This uses the chain rule to propagate the error gradient backward.\n",
            "\n",
            "##### Step 3.3: Compute gradients for weights and biases\n",
            "\n",
            "\\[\n",
            "\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T\n",
            "\\]\n",
            "\\[\n",
            "\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} = \\delta^{(l)}\n",
            "\\]\n",
            "\n",
            "- These gradients indicate how to adjust weights and biases to reduce loss.\n",
            "\n",
            "#### 4. Update Parameters\n",
            "\n",
            "- Using an optimization algorithm, typically Gradient Descent or variants (SGD, Adam, etc.), update weights and biases:\n",
            "\n",
            "\\[\n",
            "W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
            "\\]\n",
            "\\[\n",
            "b^{(l)} \\leftarrow b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
            "\\]\n",
            "\n",
            "- \\( \\eta \\) is the learning rate.\n",
            "\n",
            "---\n",
            "\n",
            "### Intuition Behind Backpropagation\n",
            "\n",
            "- The forward pass computes the networkâ€™s prediction.\n",
            "- The loss quantifies how far this prediction is from the truth.\n",
            "- Backpropagation efficiently computes how each weight contributed to the error.\n",
            "- By adjusting weights in the direction that reduces error, the network â€œlearnsâ€ to make better predictions.\n",
            "\n",
            "---\n",
            "\n",
            "### Mathematical Foundation: Chain Rule\n",
            "\n",
            "Backpropagation is fundamentally an application of the **chain rule** from calculus to compute gradients of composite functions (the layers of the network).\n",
            "\n",
            "---\n",
            "\n",
            "### Summary\n",
            "\n",
            "- **Backpropagation** enables neural networks to learn by:\n",
            "  - Performing a forward pass to compute predictions.\n",
            "  - Calculating the loss to measure prediction error.\n",
            "  - Propagating this error backward through the network to compute gradients.\n",
            "  - Updating weights and biases using these gradients to minimize the loss.\n",
            "\n",
            "This iterative process continues over many training examples (epochs), gradually improving the networkâ€™s performance.\n",
            "\n",
            "---\n",
            "\n",
            "If you want, I can also provide a code example illustrating backpropagation in practice.\n",
            "\n",
            "============================================================\n",
            "Query: Write a short poem about the stars\n",
            "============================================================\n",
            "ðŸ”€ Router Agent: Analyzing query...\n",
            "âœ… Query routed to: creative\n",
            "\n",
            "ðŸŽ¨ Creative Agent: Processing query...\n",
            "\n",
            "[Creative Agent] Beneath the velvet sky they gleam,  \n",
            "Whispering secrets from a dream,  \n",
            "Silent dancers, bright and far,  \n",
            "Each a wish, a glowing star.  \n",
            "\n",
            "They paint the night with silver streams,  \n",
            "Guiding souls through midnightâ€™s dreams,  \n",
            "In their light, the worldâ€™s ajarâ€”  \n",
            "Endless tales told by each star.\n",
            "\n",
            "============================================================\n",
            "Query: What's the best way to stay productive?\n",
            "============================================================\n",
            "ðŸ”€ Router Agent: Analyzing query...\n",
            "âœ… Query routed to: general\n",
            "\n",
            "ðŸ“ General Agent: Processing query...\n",
            "\n",
            "[General Agent] Staying productive often depends on your personal preferences and the nature of your work, but here are some widely effective strategies that can help:\n",
            "\n",
            "1. **Set Clear Goals:** Define what you want to achieve each day or week. Breaking larger tasks into smaller, manageable steps can make them less overwhelming and easier to tackle.\n",
            "\n",
            "2. **Prioritize Tasks:** Use methods like the Eisenhower Matrix or the Pomodoro Technique to focus on important and urgent tasks first, ensuring you spend time on what truly matters.\n",
            "\n",
            "3. **Create a Routine:** Establishing consistent work habits and a dedicated workspace can help signal your brain that itâ€™s time to focus.\n",
            "\n",
            "4. **Minimize Distractions:** Turn off non-essential notifications, limit social media use, and create an environment conducive to concentration.\n",
            "\n",
            "5. **Take Regular Breaks:** Short breaks can help maintain focus and prevent burnout. Techniques like working for 25 minutes followed by a 5-minute break (Pomodoro Technique) are popular.\n",
            "\n",
            "6. **Stay Healthy:** Adequate sleep, regular exercise, and good nutrition can have a significant impact on your energy levels and cognitive function.\n",
            "\n",
            "7. **Reflect and Adjust:** At the end of the day or week, review what worked well and what didnâ€™t, then adjust your approach accordingly.\n",
            "\n",
            "Remember, productivity isnâ€™t about working nonstop but working smartly and sustainably. Experiment with different techniques to find what best fits your style and needs.\n"
          ]
        }
      ],
      "source": [
        "# Test the Routing pattern with different query types\n",
        "test_queries = [\n",
        "    \"Explain how neural networks learn through backpropagation\",\n",
        "    \"Write a short poem about the stars\",\n",
        "    \"What's the best way to stay productive?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    result = route_app.invoke({\n",
        "        \"query\": query,\n",
        "        \"route\": \"\",\n",
        "        \"response\": \"\"\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{result['response']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee5499e0",
      "metadata": {
        "id": "ee5499e0"
      },
      "source": [
        "## Pattern C: Parallelization\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Parallelization** runs multiple agents or tasks simultaneously and then aggregates their results. This significantly improves performance for independent tasks.\n",
        "\n",
        "**Use Cases:**\n",
        "- Multi-perspective analysis\n",
        "- Gathering information from multiple sources\n",
        "- A/B testing different approaches\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "         â”Œâ”€â”€> Agent A â”€â”€â”\n",
        "Input â”€â”€â”€â”¼â”€â”€> Agent B â”€â”€â”¼â”€â”€> Aggregator â†’ Output\n",
        "         â””â”€â”€> Agent C â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "450fa284",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450fa284",
        "outputId": "cee91411-5ba3-4902-c725-6f43c583fe4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Parallelization workflow created!\n"
          ]
        }
      ],
      "source": [
        "# Define the state structure for parallelization\n",
        "# Using Annotated with operator.add to safely handle concurrent updates\n",
        "class ParallelState(TypedDict):\n",
        "    topic: str\n",
        "    perspective_1: str  # Technical perspective\n",
        "    perspective_2: str  # Business perspective\n",
        "    perspective_3: str  # Ethical perspective\n",
        "    summary: str\n",
        "    # Track which perspectives are complete\n",
        "    completed: Annotated[list[str], operator.add]\n",
        "\n",
        "# Parallel Agent 1: Technical Perspective\n",
        "def technical_perspective(state: ParallelState) -> ParallelState:\n",
        "    \"\"\"\n",
        "    Analyzes the topic from a technical perspective.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”§ Agent 1: Analyzing technical aspects...\")\n",
        "\n",
        "    model = get_model(temperature=0.6)\n",
        "    prompt = f\"\"\"Analyze this topic from a TECHNICAL perspective (2-3 sentences):\n",
        "\n",
        "{state['topic']}\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    print(\"âœ… Technical analysis complete\")\n",
        "    return {\n",
        "        \"perspective_1\": response.content,\n",
        "        \"completed\": [\"technical\"]\n",
        "    }\n",
        "\n",
        "# Parallel Agent 2: Business Perspective\n",
        "def business_perspective(state: ParallelState) -> ParallelState:\n",
        "    \"\"\"\n",
        "    Analyzes the topic from a business perspective.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ’¼ Agent 2: Analyzing business aspects...\")\n",
        "\n",
        "    model = get_model(temperature=0.6)\n",
        "    prompt = f\"\"\"Analyze this topic from a BUSINESS perspective (2-3 sentences):\n",
        "\n",
        "{state['topic']}\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    print(\"âœ… Business analysis complete\")\n",
        "    return {\n",
        "        \"perspective_2\": response.content,\n",
        "        \"completed\": [\"business\"]\n",
        "    }\n",
        "\n",
        "# Parallel Agent 3: Ethical Perspective\n",
        "def ethical_perspective(state: ParallelState) -> ParallelState:\n",
        "    \"\"\"\n",
        "    Analyzes the topic from an ethical perspective.\n",
        "    \"\"\"\n",
        "    print(\"âš–ï¸ Agent 3: Analyzing ethical aspects...\")\n",
        "\n",
        "    model = get_model(temperature=0.6)\n",
        "    prompt = f\"\"\"Analyze this topic from an ETHICAL perspective (2-3 sentences):\n",
        "\n",
        "{state['topic']}\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    print(\"âœ… Ethical analysis complete\")\n",
        "    return {\n",
        "        \"perspective_3\": response.content,\n",
        "        \"completed\": [\"ethical\"]\n",
        "    }\n",
        "\n",
        "# Aggregator Agent: Combines all perspectives\n",
        "def aggregate_perspectives(state: ParallelState) -> ParallelState:\n",
        "    \"\"\"\n",
        "    Aggregates all parallel analyses into a comprehensive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ”„ Aggregator: Combining all perspectives...\")\n",
        "\n",
        "    model = get_model(temperature=0.7)\n",
        "    prompt = f\"\"\"Create a comprehensive summary by combining these three perspectives:\n",
        "\n",
        "Technical: {state['perspective_1']}\n",
        "\n",
        "Business: {state['perspective_2']}\n",
        "\n",
        "Ethical: {state['perspective_3']}\n",
        "\n",
        "Summary (3-4 sentences):\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"summary\"] = response.content\n",
        "    print(\"âœ… Summary created\")\n",
        "    return state\n",
        "\n",
        "# Build the parallel graph\n",
        "workflow_parallel = StateGraph(ParallelState)\n",
        "\n",
        "# Add nodes\n",
        "workflow_parallel.add_node(\"tech_agent\", technical_perspective)\n",
        "workflow_parallel.add_node(\"business_agent\", business_perspective)\n",
        "workflow_parallel.add_node(\"ethics_agent\", ethical_perspective)\n",
        "workflow_parallel.add_node(\"aggregator\", aggregate_perspectives)\n",
        "\n",
        "# Define parallel execution: All three agents run simultaneously\n",
        "workflow_parallel.add_edge(START, \"tech_agent\")\n",
        "workflow_parallel.add_edge(START, \"business_agent\")\n",
        "workflow_parallel.add_edge(START, \"ethics_agent\")\n",
        "\n",
        "# All agents feed into the aggregator\n",
        "workflow_parallel.add_edge(\"tech_agent\", \"aggregator\")\n",
        "workflow_parallel.add_edge(\"business_agent\", \"aggregator\")\n",
        "workflow_parallel.add_edge(\"ethics_agent\", \"aggregator\")\n",
        "\n",
        "workflow_parallel.add_edge(\"aggregator\", END)\n",
        "\n",
        "# Compile the graph\n",
        "parallel_app = workflow_parallel.compile()\n",
        "\n",
        "print(\"âœ… Parallelization workflow created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d1198d26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1198d26",
        "outputId": "046152c1-0ff3-4727-8e05-6e9d5e7f4efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Testing Parallelization Pattern\n",
            "============================================================\n",
            "ðŸ’¼ Agent 2: Analyzing business aspects...\n",
            "âš–ï¸ Agent 3: Analyzing ethical aspects...\n",
            "ðŸ”§ Agent 1: Analyzing technical aspects...\n",
            "âœ… Ethical analysis complete\n",
            "âœ… Technical analysis complete\n",
            "âœ… Business analysis complete\n",
            "\n",
            "ðŸ”„ Aggregator: Combining all perspectives...\n",
            "âœ… Summary created\n",
            "\n",
            "============================================================\n",
            "RESULTS FROM PARALLEL EXECUTION:\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Technical Perspective:\n",
            "Self-driving cars rely on a combination of sensors such as LiDAR, radar, and cameras to perceive their environment, coupled with advanced machine learning algorithms for object detection, localization, and decision-making. These systems integrate real-time data processing, sensor fusion, and control mechanisms to navigate complex traffic scenarios while ensuring safety and compliance with traffic regulations.\n",
            "\n",
            "ðŸ’¼ Business Perspective:\n",
            "From a business perspective, self-driving cars present significant opportunities for innovation and disruption across multiple industries, including automotive manufacturing, ride-sharing, and logistics. Companies investing in autonomous vehicle technology can gain competitive advantages through cost reductions in labor, enhanced safety features, and new revenue models such as subscription services or data monetization. However, they must also navigate regulatory challenges, high R&D costs, and potential public resistance to widespread adoption.\n",
            "\n",
            "âš–ï¸ Ethical Perspective:\n",
            "From an ethical perspective, self-driving cars raise questions about responsibility and decision-making in life-and-death scenarios, such as how the vehicle should prioritize the safety of passengers versus pedestrians. Additionally, issues of accountability arise when accidents occur, challenging traditional notions of liability between manufacturers, software developers, and users.\n",
            "\n",
            "ðŸŽ¯ Comprehensive Summary:\n",
            "Self-driving cars integrate advanced sensor technologies and machine learning algorithms to safely navigate complex environments, enabling real-time perception and decision-making. From a business standpoint, they offer transformative opportunities across industries by reducing costs and creating new revenue streams, though significant regulatory and development challenges remain. Ethically, autonomous vehicles prompt critical debates about decision-making in emergencies and the allocation of responsibility in accidents, necessitating new frameworks for accountability. Together, these technical, business, and ethical dimensions shape the future development and adoption of self-driving technology.\n"
          ]
        }
      ],
      "source": [
        "# Test the Parallelization pattern\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Parallelization Pattern\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = parallel_app.invoke({\n",
        "    \"topic\": \"Self-driving cars\",\n",
        "    \"perspective_1\": \"\",\n",
        "    \"perspective_2\": \"\",\n",
        "    \"perspective_3\": \"\",\n",
        "    \"summary\": \"\",\n",
        "    \"completed\": []\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESULTS FROM PARALLEL EXECUTION:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nðŸ“Š Technical Perspective:\\n{result['perspective_1']}\")\n",
        "print(f\"\\nðŸ’¼ Business Perspective:\\n{result['perspective_2']}\")\n",
        "print(f\"\\nâš–ï¸ Ethical Perspective:\\n{result['perspective_3']}\")\n",
        "print(f\"\\nðŸŽ¯ Comprehensive Summary:\\n{result['summary']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf63722f",
      "metadata": {
        "id": "bf63722f"
      },
      "source": [
        "## Pattern D: Orchestrator-Worker\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Orchestrator-Worker** (also called Manager-Worker) is a hierarchical pattern where a manager agent breaks down complex tasks into subtasks and delegates them to worker agents in a coordinated manner.\n",
        "\n",
        "**Use Cases:**\n",
        "- Complex project planning and execution\n",
        "- Multi-step research tasks\n",
        "- Iterative problem-solving workflows\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Manager â”€â”€> Step 1 â”€â”€> Worker â”€â”€â”\n",
        "   â”‚                            â”‚\n",
        "   â””â”€â”€> Step 2 â”€â”€> Worker â”€â”€â”€â”€â”€â”€â”¤\n",
        "   â”‚                            â”œâ”€â”€> Final Result\n",
        "   â””â”€â”€> Step 3 â”€â”€> Worker â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5bca1e4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bca1e4f",
        "outputId": "d820bfcf-3b56-40ec-971d-71d50ada827a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Orchestrator-Worker workflow created!\n"
          ]
        }
      ],
      "source": [
        "# Define the state structure for orchestrator-worker\n",
        "class OrchestratorState(TypedDict):\n",
        "    task: str\n",
        "    plan: list[str]\n",
        "    current_step: int\n",
        "    results: list[str]\n",
        "    final_report: str\n",
        "\n",
        "# Manager Agent: Creates the plan\n",
        "def manager_agent(state: OrchestratorState) -> OrchestratorState:\n",
        "    \"\"\"\n",
        "    Manager Agent: Breaks down the complex task into 3 actionable steps.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ‘” Manager Agent: Creating execution plan...\")\n",
        "\n",
        "    model = get_model(temperature=0.5)\n",
        "    prompt = f\"\"\"Break down this task into exactly 3 clear, actionable steps.\n",
        "Format: Return only 3 lines, each starting with a number.\n",
        "\n",
        "Task: {state['task']}\n",
        "\n",
        "Steps:\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    # Parse the steps\n",
        "    steps = [line.strip() for line in response.content.split('\\n') if line.strip() and line.strip()[0].isdigit()]\n",
        "    state[\"plan\"] = steps[:3]  # Ensure we have exactly 3 steps\n",
        "    state[\"current_step\"] = 0\n",
        "    state[\"results\"] = []\n",
        "\n",
        "    print(f\"âœ… Plan created with {len(state['plan'])} steps\")\n",
        "    for i, step in enumerate(state['plan'], 1):\n",
        "        print(f\"   Step {i}: {step}\")\n",
        "    return state\n",
        "\n",
        "# Worker Agent: Executes individual steps\n",
        "def worker_agent(state: OrchestratorState) -> OrchestratorState:\n",
        "    \"\"\"\n",
        "    Worker Agent: Executes the current step from the plan.\n",
        "    \"\"\"\n",
        "    current = state[\"current_step\"]\n",
        "    step = state[\"plan\"][current]\n",
        "\n",
        "    print(f\"\\nðŸ‘· Worker Agent: Executing step {current + 1}...\")\n",
        "    print(f\"   Task: {step}\")\n",
        "\n",
        "    model = get_model(temperature=0.7)\n",
        "    prompt = f\"\"\"Execute this step and provide the result (2-3 sentences):\n",
        "\n",
        "Step: {step}\n",
        "Context: This is part of the larger task: {state['task']}\n",
        "\n",
        "Result:\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"results\"].append(response.content)\n",
        "    state[\"current_step\"] += 1\n",
        "\n",
        "    print(f\"âœ… Step {current + 1} completed\")\n",
        "    return state\n",
        "\n",
        "# Decide whether to continue or finish\n",
        "def should_continue(state: OrchestratorState) -> str:\n",
        "    \"\"\"\n",
        "    Determines if there are more steps to execute.\n",
        "    \"\"\"\n",
        "    if state[\"current_step\"] < len(state[\"plan\"]):\n",
        "        return \"worker\"  # More steps to execute\n",
        "    else:\n",
        "        return \"finalizer\"  # All steps completed\n",
        "\n",
        "# Finalizer Agent: Compiles results\n",
        "def finalizer_agent(state: OrchestratorState) -> OrchestratorState:\n",
        "    \"\"\"\n",
        "    Finalizer Agent: Combines all worker results into a final report.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ“‹ Finalizer Agent: Creating final report...\")\n",
        "\n",
        "    model = get_model(temperature=0.6)\n",
        "\n",
        "    # Compile all results\n",
        "    results_text = \"\\n\\n\".join([f\"Step {i+1} Result: {result}\"\n",
        "                                 for i, result in enumerate(state[\"results\"])])\n",
        "\n",
        "    prompt = f\"\"\"Create a comprehensive final report by synthesizing these step results:\n",
        "\n",
        "{results_text}\n",
        "\n",
        "Original Task: {state['task']}\n",
        "\n",
        "Final Report (3-4 sentences):\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"final_report\"] = response.content\n",
        "\n",
        "    print(\"âœ… Final report created\")\n",
        "    return state\n",
        "\n",
        "# Build the orchestrator-worker graph\n",
        "workflow_orchestrator = StateGraph(OrchestratorState)\n",
        "\n",
        "# Add nodes\n",
        "workflow_orchestrator.add_node(\"manager\", manager_agent)\n",
        "workflow_orchestrator.add_node(\"worker\", worker_agent)\n",
        "workflow_orchestrator.add_node(\"finalizer\", finalizer_agent)\n",
        "\n",
        "# Define the flow\n",
        "workflow_orchestrator.add_edge(START, \"manager\")\n",
        "workflow_orchestrator.add_conditional_edges(\n",
        "    \"manager\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"worker\": \"worker\",\n",
        "        \"finalizer\": \"finalizer\"\n",
        "    }\n",
        ")\n",
        "# Worker can loop back to itself or go to finalizer\n",
        "workflow_orchestrator.add_conditional_edges(\n",
        "    \"worker\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"worker\": \"worker\",\n",
        "        \"finalizer\": \"finalizer\"\n",
        "    }\n",
        ")\n",
        "workflow_orchestrator.add_edge(\"finalizer\", END)\n",
        "\n",
        "# Compile the graph\n",
        "orchestrator_app = workflow_orchestrator.compile()\n",
        "\n",
        "print(\"âœ… Orchestrator-Worker workflow created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7446030b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7446030b",
        "outputId": "04ea2f00-9c62-4614-8135-749a60750460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Testing Orchestrator-Worker Pattern\n",
            "============================================================\n",
            "ðŸ‘” Manager Agent: Creating execution plan...\n",
            "âœ… Plan created with 3 steps\n",
            "   Step 1: 1. Research target audience and define campaign goals and key messages.\n",
            "   Step 2: 2. Create engaging content and schedule posts across selected social media platforms.\n",
            "   Step 3: 3. Launch the campaign, monitor performance, and adjust strategies based on analytics.\n",
            "\n",
            "ðŸ‘· Worker Agent: Executing step 1...\n",
            "   Task: 1. Research target audience and define campaign goals and key messages.\n",
            "âœ… Step 1 completed\n",
            "\n",
            "ðŸ‘· Worker Agent: Executing step 2...\n",
            "   Task: 2. Create engaging content and schedule posts across selected social media platforms.\n",
            "âœ… Step 2 completed\n",
            "\n",
            "ðŸ‘· Worker Agent: Executing step 3...\n",
            "   Task: 3. Launch the campaign, monitor performance, and adjust strategies based on analytics.\n",
            "âœ… Step 3 completed\n",
            "\n",
            "ðŸ“‹ Finalizer Agent: Creating final report...\n",
            "âœ… Final report created\n",
            "\n",
            "============================================================\n",
            "EXECUTION RESULTS:\n",
            "============================================================\n",
            "\n",
            "ðŸ“Œ Step 1 Result:\n",
            "The target audience for the new mobile app consists primarily of tech-savvy millennials aged 18-35 who are active on social media and interested in productivity and lifestyle enhancement tools. The campaign goals are to increase app downloads by 30% within three months and boost brand awareness. Key messages will focus on the appâ€™s ease of use, time-saving features, and how it improves daily productivity.\n",
            "\n",
            "ðŸ“Œ Step 2 Result:\n",
            "Engaging content was created, including short demo videos, user testimonials, and interactive polls highlighting the appâ€™s key features. These posts were scheduled across Instagram, Facebook, and Twitter at optimal times to maximize reach and user engagement over the next four weeks.\n",
            "\n",
            "ðŸ“Œ Step 3 Result:\n",
            "The campaign was successfully launched across targeted social media platforms, generating initial engagement and downloads. Continuous monitoring revealed higher interaction rates on Instagram, prompting a shift in budget allocation to boost ads there, while underperforming channels were optimized with revised content strategies.\n",
            "\n",
            "============================================================\n",
            "FINAL REPORT:\n",
            "============================================================\n",
            "The social media marketing campaign for the new mobile app effectively targeted tech-savvy millennials aged 18-35, focusing on key messages around ease of use, time-saving features, and productivity enhancement. Engaging content, including demo videos, testimonials, and interactive polls, was strategically deployed across Instagram, Facebook, and Twitter, with scheduling optimized for maximum reach. Following the campaign launch, data-driven adjustments were made by reallocating budget towards Instagram, where engagement was highest, resulting in increased app downloads and improved brand awareness. Overall, the campaign successfully advanced toward the goal of a 30% download increase within three months.\n"
          ]
        }
      ],
      "source": [
        "# Test the Orchestrator-Worker pattern\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Orchestrator-Worker Pattern\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = orchestrator_app.invoke({\n",
        "    \"task\": \"Plan and execute a social media marketing campaign for a new mobile app\",\n",
        "    \"plan\": [],\n",
        "    \"current_step\": 0,\n",
        "    \"results\": [],\n",
        "    \"final_report\": \"\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXECUTION RESULTS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, step_result in enumerate(result[\"results\"], 1):\n",
        "    print(f\"\\nðŸ“Œ Step {i} Result:\")\n",
        "    print(step_result)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL REPORT:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"final_report\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e964295",
      "metadata": {
        "id": "3e964295"
      },
      "source": [
        "## Pattern E: Evaluator-Optimizer\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Evaluator-Optimizer** implements a feedback loop where one agent generates a solution, another agent evaluates it and provides critique, and the generator improves the solution based on the feedback.\n",
        "\n",
        "**Use Cases:**\n",
        "- Code review and improvement\n",
        "- Content refinement\n",
        "- Iterative problem-solving\n",
        "- Quality assurance workflows\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Generator â”€â”€> Draft â”€â”€> Evaluator â”€â”€> Critique\n",
        "    â†‘                                    â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Improvement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f999def6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f999def6",
        "outputId": "bdc23a01-9251-4f6b-f473-492bcb966591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Evaluator-Optimizer workflow created!\n"
          ]
        }
      ],
      "source": [
        "# Define the state structure for evaluator-optimizer\n",
        "class EvaluatorState(TypedDict):\n",
        "    task: str\n",
        "    solution: str\n",
        "    critique: str\n",
        "    iteration: int\n",
        "    max_iterations: int\n",
        "    is_approved: bool\n",
        "\n",
        "# Generator Agent: Creates or improves the solution\n",
        "def generator_agent(state: EvaluatorState) -> EvaluatorState:\n",
        "    \"\"\"\n",
        "    Generator Agent: Creates initial solution or improves based on critique.\n",
        "    \"\"\"\n",
        "    iteration = state[\"iteration\"]\n",
        "\n",
        "    if iteration == 0:\n",
        "        print(\"ðŸŽ¨ Generator Agent: Creating initial solution...\")\n",
        "        model = get_model(temperature=0.8)\n",
        "        prompt = f\"\"\"Create a solution for this task (keep it short, 2-3 sentences):\n",
        "\n",
        "{state['task']}\n",
        "\n",
        "Solution:\"\"\"\n",
        "    else:\n",
        "        print(f\"ðŸ”§ Generator Agent: Improving solution (Iteration {iteration})...\")\n",
        "        model = get_model(temperature=0.7)\n",
        "        prompt = f\"\"\"Improve this solution based on the critique:\n",
        "\n",
        "Original Solution: {state['solution']}\n",
        "\n",
        "Critique: {state['critique']}\n",
        "\n",
        "Improved Solution (keep it short, 2-3 sentences):\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    state[\"solution\"] = response.content\n",
        "    state[\"iteration\"] += 1\n",
        "\n",
        "    print(f\"âœ… Solution {'created' if iteration == 0 else 'improved'}\")\n",
        "    return state\n",
        "\n",
        "# Evaluator Agent: Reviews and critiques the solution\n",
        "def evaluator_agent(state: EvaluatorState) -> EvaluatorState:\n",
        "    \"\"\"\n",
        "    Evaluator Agent: Reviews the solution and provides critique.\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ” Evaluator Agent: Reviewing solution...\")\n",
        "\n",
        "    model = get_model(temperature=0.5)\n",
        "    prompt = f\"\"\"Evaluate this solution and provide feedback.\n",
        "\n",
        "Task: {state['task']}\n",
        "Solution: {state['solution']}\n",
        "\n",
        "Provide your evaluation:\n",
        "1. Is it good enough? (YES/NO)\n",
        "2. If NO, what specific improvements are needed?\n",
        "\n",
        "Start your response with 'APPROVED' if it's good enough, or 'NEEDS IMPROVEMENT' if not.\n",
        "\n",
        "Evaluation:\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    critique = response.content\n",
        "    state[\"critique\"] = critique\n",
        "\n",
        "    # Check if approved\n",
        "    if \"APPROVED\" in critique.upper().split('\\n')[0]:\n",
        "        state[\"is_approved\"] = True\n",
        "        print(\"âœ… Solution APPROVED!\")\n",
        "    else:\n",
        "        state[\"is_approved\"] = False\n",
        "        print(\"âŒ Needs improvement\")\n",
        "\n",
        "    return state\n",
        "\n",
        "# Decide whether to continue improving or finish\n",
        "def should_continue_optimization(state: EvaluatorState) -> str:\n",
        "    \"\"\"\n",
        "    Determines if we should continue iterating or finish.\n",
        "    \"\"\"\n",
        "    # Stop if approved or reached max iterations\n",
        "    if state[\"is_approved\"]:\n",
        "        return \"end\"\n",
        "    elif state[\"iteration\"] >= state[\"max_iterations\"]:\n",
        "        print(f\"\\nâš ï¸ Max iterations ({state['max_iterations']}) reached\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        return \"generator\"  # Continue improving\n",
        "\n",
        "# Build the evaluator-optimizer graph\n",
        "workflow_evaluator = StateGraph(EvaluatorState)\n",
        "\n",
        "# Add nodes\n",
        "workflow_evaluator.add_node(\"generator\", generator_agent)\n",
        "workflow_evaluator.add_node(\"evaluator\", evaluator_agent)\n",
        "\n",
        "# Define the flow\n",
        "workflow_evaluator.add_edge(START, \"generator\")\n",
        "workflow_evaluator.add_edge(\"generator\", \"evaluator\")\n",
        "workflow_evaluator.add_conditional_edges(\n",
        "    \"evaluator\",\n",
        "    should_continue_optimization,\n",
        "    {\n",
        "        \"generator\": \"generator\",  # Loop back for improvement\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile the graph\n",
        "evaluator_app = workflow_evaluator.compile()\n",
        "\n",
        "print(\"âœ… Evaluator-Optimizer workflow created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6d00745f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d00745f",
        "outputId": "0463ece8-ddb5-4521-d7fe-b16671916f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Testing Evaluator-Optimizer Pattern\n",
            "============================================================\n",
            "ðŸŽ¨ Generator Agent: Creating initial solution...\n",
            "âœ… Solution created\n",
            "\n",
            "ðŸ” Evaluator Agent: Reviewing solution...\n",
            "âœ… Solution APPROVED!\n",
            "\n",
            "============================================================\n",
            "OPTIMIZATION RESULTS:\n",
            "============================================================\n",
            "\n",
            "Total Iterations: 1\n",
            "Status: âœ… APPROVED\n",
            "\n",
            "ðŸŽ¯ Final Solution:\n",
            "Stay refreshed, save the planet â€” sip sustainably with our eco-friendly bottles.\n",
            "\n",
            "ðŸ“ Final Evaluation:\n",
            "APPROVED\n",
            "\n",
            "The tagline is clear, engaging, and effectively communicates both the product benefit (\"Stay refreshed\") and the eco-friendly mission (\"save the planet,\" \"sip sustainably\"). It uses a nice rhythm and alliteration that make it memorable. Overall, it is compelling and appropriate for an eco-friendly water bottle company.\n"
          ]
        }
      ],
      "source": [
        "# Test the Evaluator-Optimizer pattern\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Evaluator-Optimizer Pattern\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = evaluator_app.invoke({\n",
        "    \"task\": \"Write a compelling tagline for an eco-friendly water bottle company\",\n",
        "    \"solution\": \"\",\n",
        "    \"critique\": \"\",\n",
        "    \"iteration\": 0,\n",
        "    \"max_iterations\": 3,\n",
        "    \"is_approved\": False\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OPTIMIZATION RESULTS:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal Iterations: {result['iteration']}\")\n",
        "print(f\"Status: {'âœ… APPROVED' if result['is_approved'] else 'âš ï¸ Max iterations reached'}\")\n",
        "print(f\"\\nðŸŽ¯ Final Solution:\\n{result['solution']}\")\n",
        "print(f\"\\nðŸ“ Final Evaluation:\\n{result['critique']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1bb2433",
      "metadata": {
        "id": "a1bb2433"
      },
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Pattern Selection Guide\n",
        "\n",
        "Choosing the right agentic pattern depends on your specific use case:\n",
        "\n",
        "### When to Use Each Pattern\n",
        "\n",
        "| Pattern | Best For | Key Benefit |\n",
        "|---------|----------|-------------|\n",
        "| **Prompt Chaining** | Sequential transformations, multi-stage content generation | Simplicity and clarity |\n",
        "| **Routing** | Multi-domain problems, task classification | Specialization and accuracy |\n",
        "| **Parallelization** | Independent tasks, multi-perspective analysis | Speed and efficiency |\n",
        "| **Orchestrator-Worker** | Complex planning, multi-step execution | Coordination and structure |\n",
        "| **Evaluator-Optimizer** | Quality-critical tasks, iterative refinement | Quality and improvement |\n",
        "\n",
        "### Combining Patterns\n",
        "\n",
        "These patterns can be combined for even more powerful workflows:\n",
        "\n",
        "- **Routing + Prompt Chaining**: Route to specialized chains\n",
        "- **Orchestrator + Parallelization**: Manager delegates parallel tasks\n",
        "- **Parallelization + Evaluator**: Multiple solutions evaluated in parallel\n",
        "- **All patterns together**: Complex enterprise systems\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Start Simple**: Begin with basic patterns and add complexity as needed\n",
        "2. **State Management**: Langgraph's state management is crucial for complex workflows\n",
        "3. **Error Handling**: Always implement proper error handling in production\n",
        "4. **Testing**: Test each agent independently before combining\n",
        "5. **Monitoring**: Track agent performance and iterations\n",
        "6. **Cost Management**: Be mindful of API calls in loops and parallel operations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different combinations of patterns\n",
        "- Add memory and persistence to your agents\n",
        "- Integrate external tools and APIs\n",
        "- Build domain-specific multi-agent systems\n",
        "- Implement proper logging and monitoring\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Langgraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for completing this tutorial!** ðŸŽ‰\n",
        "\n",
        "Feel free to adapt these patterns for your own projects. Happy building! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}