{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3c23f4f",
      "metadata": {
        "id": "a3c23f4f"
      },
      "source": [
        "# Multi-Agent Systems and Agentic Patterns with LlamaIndex\n",
        "\n",
        "## Tutorial Overview\n",
        "\n",
        "In this comprehensive tutorial, we'll explore **multi-agent systems** and **agentic patterns** using **LlamaIndex** with **OpenAI's gpt-4o-mini** model as our LLM backend.\n",
        "\n",
        "---\n",
        "\n",
        "## What is LlamaIndex?\n",
        "\n",
        "**LlamaIndex** is a powerful data framework designed to help you build context-augmented LLM applications. It provides:\n",
        "\n",
        "- **Data Connectors**: Ingest data from various sources (APIs, PDFs, databases, etc.)\n",
        "- **Data Indexes**: Structure your data for efficient retrieval\n",
        "- **Agents**: Intelligent decision-making components that can use tools and reason about tasks\n",
        "- **Query Engines**: Retrieve relevant context and generate answers\n",
        "- **Chat Engines**: Build conversational interfaces\n",
        "\n",
        "---\n",
        "\n",
        "## Why are Agentic Patterns Important?\n",
        "\n",
        "**Agentic patterns** enable AI systems to:\n",
        "\n",
        "1. **Break down complex problems** into manageable sub-tasks\n",
        "2. **Make autonomous decisions** about which tools or approaches to use\n",
        "3. **Coordinate multiple specialized agents** for better outcomes\n",
        "4. **Self-improve** through feedback loops and evaluation\n",
        "5. **Scale reasoning** beyond single-shot prompting\n",
        "\n",
        "In production systems, these patterns help create more robust, maintainable, and powerful AI applications.\n",
        "\n",
        "---\n",
        "\n",
        "## Patterns We'll Cover\n",
        "\n",
        "1. **Prompt Chaining**: Sequential agent communication\n",
        "2. **Routing**: Intelligent task delegation\n",
        "3. **Parallelization**: Concurrent execution and aggregation\n",
        "4. **Orchestrator-Worker**: Hierarchical task management\n",
        "5. **Evaluator-Optimizer**: Iterative improvement through feedback\n",
        "\n",
        "Let's get started! ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756db374",
      "metadata": {
        "id": "756db374"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Setup & Installation\n",
        "\n",
        "First, let's install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb1ada21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb1ada21",
        "outputId": "612d7fc7-f6ec-4a1d-8080-b7fdd783646c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install llama-index llama-index-llms-openai openai python-dotenv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2c409f",
      "metadata": {
        "id": "6f2c409f"
      },
      "source": [
        "### Configure OpenAI API Key\n",
        "\n",
        "You'll need an OpenAI API key. Get one from: https://platform.openai.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7ff9aa97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ff9aa97",
        "outputId": "ede49bd4-8ad6-4e65-8c96-66f4d99a70bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your OpenAI Base URL: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "if \"OPENAI_BASE_URL\" not in os.environ:\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = getpass.getpass(\"Enter your OpenAI Base URL: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e120b46",
      "metadata": {
        "id": "4e120b46"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Model Configuration\n",
        "\n",
        "Let's set up our OpenAI configuration using the **gpt-4o-mini** model. We'll create a reusable setup that we can use throughout the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ff8f86e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ff8f86e",
        "outputId": "438bb18a-47d2-460f-ddea-8da1850734b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model configuration complete!\n",
            "ðŸ“‹ LLM: gpt-4.1-mini (via OpenAI)\n",
            "ðŸ“‹ API Base: https://ds-ai-internship.openai.azure.com/openai/v1/\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Configure OpenAI\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
        "    temperature=0.7,\n",
        "    max_tokens=2048,\n",
        ")\n",
        "\n",
        "# Set global defaults for LlamaIndex\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 512\n",
        "\n",
        "print(\"âœ… Model configuration complete!\")\n",
        "print(f\"ðŸ“‹ LLM: gpt-4.1-mini (via OpenAI)\")\n",
        "print(f\"ðŸ“‹ API Base: {os.getenv('OPENAI_BASE_URL')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "47fa4915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47fa4915",
        "outputId": "13a881b2-e6fd-4b03-ad0e-03cc84df909d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: Yes, I'm ready!\n"
          ]
        }
      ],
      "source": [
        "# Test the model configuration\n",
        "response = llm.complete(\"Hello! Can you confirm you're working? Reply with just 'Yes, I'm ready!'\")\n",
        "print(f\"Model Response: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27920be3",
      "metadata": {
        "id": "27920be3"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Agentic Patterns\n",
        "\n",
        "Now let's explore each pattern with practical examples!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4405ccf5",
      "metadata": {
        "id": "4405ccf5"
      },
      "source": [
        "---\n",
        "\n",
        "### Pattern A: Prompt Chaining\n",
        "\n",
        "**Concept**: The output of one agent becomes the input of another agent. This creates a sequential pipeline where each step builds on the previous one.\n",
        "\n",
        "**Use Cases**:\n",
        "- Content creation pipelines (outline â†’ draft â†’ polish)\n",
        "- Data transformation workflows\n",
        "- Multi-step reasoning tasks\n",
        "\n",
        "**Implementation**: We'll create Agent A (Story Generator) and Agent B (Story Improver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2ceec117",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ceec117",
        "outputId": "064a0fbe-e638-47c0-db95-3de21c18aef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ”— PROMPT CHAINING EXAMPLE\n",
            "============================================================\n",
            "\n",
            "ðŸ¤– Agent A (Story Generator) is working...\n",
            "\n",
            "ðŸ“ Agent A Output:\n",
            "A small robot named Pixel watched colorful paintings in a gallery and wanted to create art too. It picked up a brush and began mixing bright colors on a canvas. At first, its strokes were shaky, but with practice, Pixel painted a beautiful sunset. Proud of its work, the robot smiled, knowing it had learned to express itself through painting.\n",
            "\n",
            "\n",
            "ðŸ¤– Agent B (Story Improver) is working...\n",
            "\n",
            "âœ¨ Agent B Output:\n",
            "In the heart of a bustling gallery filled with vibrant masterpieces, a small robot named Pixel stood quietly, its optical sensors gleaming with wonder. The walls were alive with splashes of crimson, gold, and indigoâ€”each painting whispering stories through swirling brushstrokes and radiant hues. Mesmerized, Pixel felt a stirring deep within its circuits, a yearning to create its own symphony of color.\n",
            "\n",
            "With determination, Pixel gently picked up a slender brush, its metal fingers surprisingly delicate. It dipped the brush into vivid pots of paintâ€”fiery reds, glowing oranges, and soft purplesâ€”and tentatively began to glide strokes across a blank canvas. At first, the lines wavered, uncertain and uneven, like a fledgling bird testing its wings. But with each pass, Pixelâ€™s confidence blossomed, its movements growing smoother and more intentional.\n",
            "\n",
            "Hours melted away as the canvas transformed beneath Pixelâ€™s touch. Warm hues blended seamlessly, capturing the breathtaking glow of a sunset melting into twilight. When the final stroke was laid down, the painting radiated a quiet beautyâ€”an echo of the galleryâ€™s magic, yet undeniably Pixelâ€™s own.\n",
            "\n",
            "A soft, satisfied hum vibrated through the little robotâ€™s frame as it stepped back to admire its work. For the first time, Pixel understood the joy of creationâ€”not just the mechanical act, but the soulful expression behind every color, every line. With a bright, digital smile illuminating its face, Pixel knew it had found a new language to speak: the language of art.\n",
            "\n",
            "============================================================\n",
            "âœ… Prompt Chain Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "class PromptChainExample:\n",
        "    \"\"\"Demonstrates prompt chaining with two sequential agents.\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def agent_a_generate_story(self, topic: str) -> str:\n",
        "        \"\"\"\n",
        "        Agent A: Generates a basic story outline.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸ¤– Agent A (Story Generator) is working...\")\n",
        "\n",
        "        prompt = f\"\"\"You are a creative story generator.\n",
        "        Create a brief, simple story (3-4 sentences) about: {topic}\n",
        "        Keep it concise and straightforward.\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        story = response.message.content\n",
        "        print(f\"\\nðŸ“ Agent A Output:\\n{story}\")\n",
        "        return story\n",
        "\n",
        "    def agent_b_improve_story(self, story: str) -> str:\n",
        "        \"\"\"\n",
        "        Agent B: Takes the story from Agent A and improves it.\n",
        "        \"\"\"\n",
        "        print(\"\\n\\nðŸ¤– Agent B (Story Improver) is working...\")\n",
        "\n",
        "        prompt = f\"\"\"You are a story editor. Take this story and enhance it by:\n",
        "        1. Adding more vivid descriptions\n",
        "        2. Improving the narrative flow\n",
        "        3. Making it more engaging\n",
        "\n",
        "        Original Story:\n",
        "        {story}\n",
        "\n",
        "        Provide the improved version:\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        improved_story = response.message.content\n",
        "        print(f\"\\nâœ¨ Agent B Output:\\n{improved_story}\")\n",
        "        return improved_story\n",
        "\n",
        "    def run_chain(self, topic: str):\n",
        "        \"\"\"\n",
        "        Execute the full prompt chain: Agent A â†’ Agent B\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ðŸ”— PROMPT CHAINING EXAMPLE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: Agent A generates the story\n",
        "        story = self.agent_a_generate_story(topic)\n",
        "\n",
        "        # Step 2: Agent B improves the story (chaining the output)\n",
        "        improved_story = self.agent_b_improve_story(story)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… Prompt Chain Complete!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return improved_story\n",
        "\n",
        "# Run the example\n",
        "chain_example = PromptChainExample(llm)\n",
        "final_story = chain_example.run_chain(\"a robot learning to paint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "828c5720",
      "metadata": {
        "id": "828c5720"
      },
      "source": [
        "---\n",
        "\n",
        "### Pattern B: Routing\n",
        "\n",
        "**Concept**: A router agent analyzes the input and decides which specialized agent or tool should handle the request.\n",
        "\n",
        "**Use Cases**:\n",
        "- Customer service bots (route to departments)\n",
        "- Multi-domain question answering\n",
        "- Tool selection in complex systems\n",
        "\n",
        "**Implementation**: We'll create a router that directs queries to specialized agents (Math, History, or General)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b10ab79d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b10ab79d",
        "outputId": "fd5199b0-35b6-40b5-e9a7-aaf0d6874797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ”€ ROUTING PATTERN EXAMPLE\n",
            "============================================================\n",
            "\n",
            "ðŸ“¥ Query: What is the square root of 144?\n",
            "\n",
            "ðŸ”€ Router Agent analyzing query...\n",
            "ðŸŽ¯ Router Decision: Route to 'math' specialist\n",
            "\n",
            "ðŸ“¤ ðŸ”¢ Math Specialist: The square root of 144 is 12.\n",
            "------------------------------------------------------------\n",
            "\n",
            "ðŸ“¥ Query: When did World War II end?\n",
            "\n",
            "ðŸ”€ Router Agent analyzing query...\n",
            "ðŸŽ¯ Router Decision: Route to 'history' specialist\n",
            "\n",
            "ðŸ“¤ ðŸ“š History Specialist: World War II ended in 1945. In Europe, it concluded with Germany's surrender on May 8, 1945 (V-E Day), and in the Pacific, it ended with Japan's surrender on September 2, 1945 (V-J Day).\n",
            "------------------------------------------------------------\n",
            "\n",
            "ðŸ“¥ Query: What is the best way to learn programming?\n",
            "\n",
            "ðŸ”€ Router Agent analyzing query...\n",
            "ðŸŽ¯ Router Decision: Route to 'general' specialist\n",
            "\n",
            "ðŸ“¤ ðŸ’¡ General Specialist: The best way to learn programming depends on your goals, learning style, and resources, but here are some widely effective strategies:\n",
            "\n",
            "1. **Start with the Basics:** Choose a beginner-friendly language like Python or JavaScript to learn fundamental programming concepts such as variables, loops, conditionals, and functions.\n",
            "\n",
            "2. **Hands-On Practice:** Write code regularly. Working on small projects, coding exercises, or challenges helps reinforce concepts better than just reading or watching tutorials.\n",
            "\n",
            "3. **Use Online Resources:** Utilize free or paid platforms like Codecademy, freeCodeCamp, LeetCode, or Coursera that offer structured courses and interactive lessons.\n",
            "\n",
            "4. **Build Projects:** Apply what you learn by building real projects that interest you. This could be a personal website, a simple game, or a data analysis script. Projects help integrate multiple concepts and keep you motivated.\n",
            "\n",
            "5. **Read and Understand Code:** Study code written by others on GitHub or open-source projects to learn different coding styles and best practices.\n",
            "\n",
            "6. **Join Communities:** Participate in programming forums, coding meetups, or online communities like Stack Overflow or Reddit. Engaging with others can provide support, feedback, and motivation.\n",
            "\n",
            "7. **Be Patient and Persistent:** Programming can be challenging. Debugging and problem-solving are part of the learning process, so stay curious and donâ€™t get discouraged by setbacks.\n",
            "\n",
            "8. **Learn Computer Science Fundamentals:** Concepts like data structures, algorithms, and problem-solving techniques are essential for becoming a proficient programmer.\n",
            "\n",
            "By combining these approaches and consistently dedicating time to practice, you can effectively learn programming and improve over time.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "âœ… Routing Demo Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import json\n",
        "\n",
        "class RoutingExample:\n",
        "    \"\"\"Demonstrates routing pattern with specialized agents.\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def math_specialist(self, query: str) -> str:\n",
        "        \"\"\"Specialized agent for mathematical queries.\"\"\"\n",
        "        prompt = f\"You are a math expert. Answer this mathematical question concisely: {query}\"\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "        return f\"ðŸ”¢ Math Specialist: {response.message.content}\"\n",
        "\n",
        "    def history_specialist(self, query: str) -> str:\n",
        "        \"\"\"Specialized agent for historical queries.\"\"\"\n",
        "        prompt = f\"You are a history expert. Answer this historical question concisely: {query}\"\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "        return f\"ðŸ“š History Specialist: {response.message.content}\"\n",
        "\n",
        "    def general_specialist(self, query: str) -> str:\n",
        "        \"\"\"General purpose agent for other queries.\"\"\"\n",
        "        prompt = f\"Answer this question: {query}\"\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "        return f\"ðŸ’¡ General Specialist: {response.message.content}\"\n",
        "\n",
        "    def route_query(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Router agent that determines which specialist should handle the query.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸ”€ Router Agent analyzing query...\")\n",
        "\n",
        "        # Router decides which agent to use\n",
        "        routing_prompt = f\"\"\"Analyze this query and determine the best specialist:\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Options:\n",
        "        - 'math': For mathematical calculations, equations, or number problems\n",
        "        - 'history': For historical events, dates, or historical figures\n",
        "        - 'general': For everything else\n",
        "\n",
        "        Respond with ONLY one word: math, history, or general\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=routing_prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "        route_decision = response.message.content.strip().lower()\n",
        "\n",
        "        print(f\"ðŸŽ¯ Router Decision: Route to '{route_decision}' specialist\\n\")\n",
        "\n",
        "        # Route to appropriate specialist\n",
        "        if 'math' in route_decision:\n",
        "            return self.math_specialist(query)\n",
        "        elif 'history' in route_decision:\n",
        "            return self.history_specialist(query)\n",
        "        else:\n",
        "            return self.general_specialist(query)\n",
        "\n",
        "    def demo(self):\n",
        "        \"\"\"Demonstrate routing with different types of queries.\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ðŸ”€ ROUTING PATTERN EXAMPLE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        queries = [\n",
        "            \"What is the square root of 144?\",\n",
        "            \"When did World War II end?\",\n",
        "            \"What is the best way to learn programming?\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nðŸ“¥ Query: {query}\")\n",
        "            result = self.route_query(query)\n",
        "            print(f\"ðŸ“¤ {result}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… Routing Demo Complete!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "# Run the example\n",
        "routing_example = RoutingExample(llm)\n",
        "routing_example.demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53edd7ad",
      "metadata": {
        "id": "53edd7ad"
      },
      "source": [
        "---\n",
        "\n",
        "### Pattern C: Parallelization\n",
        "\n",
        "**Concept**: Run multiple agents or tasks simultaneously and aggregate their results.\n",
        "\n",
        "**Use Cases**:\n",
        "- Multi-perspective analysis\n",
        "- Concurrent research tasks\n",
        "- Ensemble approaches\n",
        "\n",
        "**Implementation**: We'll analyze a topic from multiple perspectives in parallel, then synthesize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "827955c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "827955c5",
        "outputId": "5d04d7af-076b-4c27-a01c-49a88e8bd130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "âš¡ PARALLELIZATION PATTERN EXAMPLE\n",
            "============================================================\n",
            "\n",
            "ðŸš€ Starting parallel analysis of: Artificial Intelligence in Healthcare\n",
            "ðŸ“‹ Perspectives: technical, ethical, economic\n",
            "\n",
            "ðŸ”„ Analyzing from technical perspective...\n",
            "ðŸ”„ Analyzing from ethical perspective...\n",
            "ðŸ”„ Analyzing from economic perspective...\n",
            "âœ… ethical analysis complete\n",
            "âœ… economic analysis complete\n",
            "âœ… technical analysis complete\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š INDIVIDUAL PERSPECTIVES:\n",
            "============================================================\n",
            "\n",
            "**ETHICAL PERSPECTIVE:**\n",
            "Analyzing Artificial Intelligence (AI) in Healthcare from an ethical perspective reveals several important considerations:\n",
            "\n",
            "1. **Patient Privacy and Data Security:**  \n",
            "   AI systems in healthcare often rely on large volumes of sensitive patient data. Ethically, it is crucial to ensure that this data is collected, stored, and processed with strict confidentiality and robust security measures to prevent breaches. Patients must give informed consent, understanding how their data will be used, and there should be transparency about AIâ€™s role in diagnosis or treatment.\n",
            "\n",
            "2. **Bias and Fairness:**  \n",
            "   AI algorithms can inherit biases present in their training data, leading to unequal treatment outcomes across different demographic groups. Ethically, developers and healthcare providers must work to identify, mitigate, and monitor these biases to ensure equitable care and avoid exacerbating existing health disparities.\n",
            "\n",
            "3. **Accountability and Transparency:**  \n",
            "   Decisions made or assisted by AI can significantly impact patient health. Ethically, there must be clear accountability for AI-driven decisions, with transparency about how these decisions are made. Healthcare professionals should retain ultimate responsibility, ensuring AI serves as a tool rather than a replacement for human judgment.\n",
            "\n",
            "These insights underscore the need for ethical frameworks that balance innovation with respect for patient rights and social justice in the deployment of AI in healthcare.\n",
            "\n",
            "**ECONOMIC PERSPECTIVE:**\n",
            "Certainly! Here are 2-3 key economic insights on Artificial Intelligence (AI) in Healthcare:\n",
            "\n",
            "1. **Cost Efficiency and Productivity Gains**  \n",
            "   AI technologies can significantly reduce healthcare costs by automating routine tasks such as diagnostics, administrative work, and patient monitoring. This can lead to improved operational efficiency, allowing healthcare providers to serve more patients at a lower cost per unit of care. Over time, this can help alleviate the financial burden on both healthcare systems and patients, potentially lowering insurance premiums and out-of-pocket expenses.\n",
            "\n",
            "2. **Market Expansion and Innovation Incentives**  \n",
            "   The integration of AI in healthcare stimulates innovation, leading to the development of new products and services such as personalized medicine, predictive analytics, and telemedicine platforms. This creates new market opportunities and can attract investment, fostering economic growth within the biotech and health tech sectors. Additionally, AI-driven improvements in healthcare outcomes may increase labor productivity by reducing illness-related absenteeism and enhancing population health.\n",
            "\n",
            "3. **Impact on Labor Markets and Skill Demand**  \n",
            "   While AI can displace certain low-skill jobs in healthcare (e.g., administrative roles or routine diagnostics), it also increases demand for highly skilled workers such as data scientists, AI specialists, and healthcare professionals trained to work alongside AI tools. This shift may require significant investments in education and retraining programs, influencing labor market dynamics and wage structures within the healthcare industry.\n",
            "\n",
            "These insights highlight how AI in healthcare can reshape economic structures by improving efficiency, driving innovation, and altering labor demand.\n",
            "\n",
            "**TECHNICAL PERSPECTIVE:**\n",
            "Certainly! Here are 3 key technical insights on Artificial Intelligence (AI) in Healthcare:\n",
            "\n",
            "1. **Data Integration and Interoperability Challenges:**  \n",
            "   AI systems in healthcare heavily rely on vast amounts of heterogeneous data sources such as electronic health records (EHRs), medical imaging, genomic data, and wearable sensor outputs. Technically, integrating these diverse datasets requires robust data standardization, normalization, and interoperability frameworks (e.g., HL7 FHIR standards). Achieving seamless data exchange while maintaining data quality and consistency is critical for training accurate AI models and enabling real-time clinical decision support.\n",
            "\n",
            "2. **Advanced Machine Learning Techniques for Diagnosis and Prognosis:**  \n",
            "   Deep learning, particularly convolutional neural networks (CNNs), has shown remarkable success in image-based diagnostics like radiology and pathology by accurately identifying anomalies. Additionally, reinforcement learning and natural language processing (NLP) enable AI to analyze longitudinal patient records for prognosis and personalized treatment recommendations. However, the technical challenge lies in developing models that are interpretable, generalizable across populations, and robust against noisy or incomplete clinical data.\n",
            "\n",
            "3. **Regulatory and Ethical Considerations Driving Technical Design:**  \n",
            "   AI solutions in healthcare must comply with stringent regulatory requirements (FDA, EMA) regarding safety, efficacy, and patient privacy (HIPAA, GDPR). This necessitates incorporating technical mechanisms such as explainability algorithms, audit trails, secure data encryption, and federated learning approaches that allow model training without centralized data sharing. These constraints influence the architecture and deployment strategies of AI systems to ensure trustworthiness and clinical adoption.\n",
            "\n",
            "In summary, the technical landscape of AI in healthcare is shaped by complex data integration needs, sophisticated machine learning model development, and adherence to regulatory standards that collectively drive innovation and safe deployment.\n",
            "\n",
            "ðŸ”— Synthesizing results...\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ SYNTHESIZED SUMMARY:\n",
            "============================================================\n",
            "\n",
            "AI in healthcare presents a transformative opportunity by enhancing efficiency, innovation, and personalized care through advanced data integration and machine learning techniques. However, its deployment must carefully address ethical concerns around patient privacy, bias, and accountability, while navigating economic shifts in labor demand and ensuring compliance with stringent regulatory standards. Balancing these technical, ethical, and economic dimensions is crucial for realizing AIâ€™s full potential in improving health outcomes equitably and sustainably.\n",
            "\n",
            "============================================================\n",
            "âœ… Parallelization Demo Complete! (Executed in 4.17s)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List\n",
        "import time\n",
        "\n",
        "class ParallelizationExample:\n",
        "    \"\"\"Demonstrates parallel execution of multiple agents using threading.\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def analyze_from_perspective(self, topic: str, perspective: str) -> str:\n",
        "        \"\"\"\n",
        "        Analyze a topic from a specific perspective.\n",
        "        \"\"\"\n",
        "        print(f\"ðŸ”„ Analyzing from {perspective} perspective...\")\n",
        "\n",
        "        prompt = f\"\"\"Analyze the following topic from a {perspective} perspective.\n",
        "        Provide 2-3 key insights.\n",
        "\n",
        "        Topic: {topic}\n",
        "\n",
        "        Perspective: {perspective}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)  # Synchronous call\n",
        "\n",
        "        result = f\"**{perspective.upper()} PERSPECTIVE:**\\n{response.message.content}\"\n",
        "        print(f\"âœ… {perspective} analysis complete\")\n",
        "        return result\n",
        "\n",
        "    def parallel_analyze(self, topic: str, perspectives: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Run multiple analyses in parallel using ThreadPoolExecutor.\n",
        "        \"\"\"\n",
        "        print(f\"\\nðŸš€ Starting parallel analysis of: {topic}\")\n",
        "        print(f\"ðŸ“‹ Perspectives: {', '.join(perspectives)}\\n\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel execution\n",
        "        with ThreadPoolExecutor(max_workers=len(perspectives)) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_perspective = {\n",
        "                executor.submit(self.analyze_from_perspective, topic, perspective): perspective\n",
        "                for perspective in perspectives\n",
        "            }\n",
        "\n",
        "            # Collect results as they complete\n",
        "            for future in as_completed(future_to_perspective):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                except Exception as e:\n",
        "                    perspective = future_to_perspective[future]\n",
        "                    print(f\"âŒ Error analyzing {perspective} perspective: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def synthesize_results(self, results: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Aggregate and synthesize the parallel results.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸ”— Synthesizing results...\")\n",
        "\n",
        "        combined_insights = \"\\n\\n\".join(results)\n",
        "\n",
        "        synthesis_prompt = f\"\"\"You have received multiple perspectives on a topic.\n",
        "        Synthesize these into a brief, coherent summary that captures the key themes.\n",
        "\n",
        "        Perspectives:\n",
        "        {combined_insights}\n",
        "\n",
        "        Provide a 2-3 sentence synthesis:\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=synthesis_prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        return response.message.content\n",
        "\n",
        "    def run_demo(self):\n",
        "        \"\"\"Demonstrate parallel execution.\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"âš¡ PARALLELIZATION PATTERN EXAMPLE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        topic = \"Artificial Intelligence in Healthcare\"\n",
        "        perspectives = [\"technical\", \"ethical\", \"economic\"]\n",
        "\n",
        "        # Track time for demonstration\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run parallel analyses\n",
        "        results = self.parallel_analyze(topic, perspectives)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Display individual results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ“Š INDIVIDUAL PERSPECTIVES:\")\n",
        "        print(\"=\"*60)\n",
        "        for result in results:\n",
        "            print(f\"\\n{result}\")\n",
        "\n",
        "        # Synthesize\n",
        "        synthesis = self.synthesize_results(results)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŽ¯ SYNTHESIZED SUMMARY:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\n{synthesis}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"âœ… Parallelization Demo Complete! (Executed in {elapsed_time:.2f}s)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "# Run the example\n",
        "parallel_example = ParallelizationExample(llm)\n",
        "parallel_example.run_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c4549dd",
      "metadata": {
        "id": "6c4549dd"
      },
      "source": [
        "---\n",
        "\n",
        "### Pattern D: Orchestrator-Worker\n",
        "\n",
        "**Concept**: A manager/orchestrator agent breaks down complex tasks and assigns sub-tasks to worker agents.\n",
        "\n",
        "**Use Cases**:\n",
        "- Project planning and execution\n",
        "- Complex research tasks\n",
        "- Multi-step content generation\n",
        "\n",
        "**Implementation**: An orchestrator plans a blog post, then delegates writing tasks to worker agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "085b6415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "085b6415",
        "outputId": "0799a044-8014-4f55-fdcd-6b210a1bbd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ‘” ORCHESTRATOR-WORKER PATTERN EXAMPLE\n",
            "============================================================\n",
            "\n",
            "ðŸ“Œ Main Task: Write a short blog post about the benefits of meditation\n",
            "\n",
            "ðŸŽ¯ Orchestrator Agent: Planning task breakdown...\n",
            "\n",
            "ðŸ“‹ Orchestrator's Plan:\n",
            "  1. Research and gather information on the benefits of meditation\n",
            "  2. Write a concise and engaging draft of the blog post\n",
            "  3. Edit and finalize the blog post for clarity and flow\n",
            "\n",
            "ðŸ‘· Worker Agent 1: Executing sub-task...\n",
            "âœ… Worker 1 completed\n",
            "\n",
            "ðŸ‘· Worker Agent 2: Executing sub-task...\n",
            "âœ… Worker 2 completed\n",
            "\n",
            "ðŸ‘· Worker Agent 3: Executing sub-task...\n",
            "âœ… Worker 3 completed\n",
            "\n",
            "ðŸŽ¯ Orchestrator Agent: Aggregating results...\n",
            "\n",
            "============================================================\n",
            "ðŸŽ‰ FINAL OUTPUT:\n",
            "============================================================\n",
            "\n",
            "Meditation offers numerous benefits that can significantly enhance your quality of life. By incorporating this simple practice into your daily routine, you can reduce stress, improve emotional health, and boost focus and concentration. Additionally, meditation promotes better sleep, lowers blood pressure, and increases self-awareness, contributing to overall well-being.\n",
            "\n",
            "Discover how these small, mindful habits can transform your productivity and create a balanced lifestyle. By managing stress effectively and fostering mental clarity, meditation helps fuel both success and happiness. Start implementing meditation today to unlock your full potential and enjoy a healthier, more centered life.\n",
            "\n",
            "============================================================\n",
            "âœ… Orchestrator-Worker Demo Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict\n",
        "import json\n",
        "\n",
        "class OrchestratorWorkerExample:\n",
        "    \"\"\"Demonstrates orchestrator-worker pattern.\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def orchestrator_plan(self, task: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Orchestrator Agent: Breaks down a complex task into sub-tasks.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸŽ¯ Orchestrator Agent: Planning task breakdown...\")\n",
        "\n",
        "        planning_prompt = f\"\"\"You are a project orchestrator. Break down this task into 3 clear sub-tasks.\n",
        "\n",
        "        Main Task: {task}\n",
        "\n",
        "        Return your response in this exact JSON format:\n",
        "        {{\n",
        "            \"subtasks\": [\n",
        "                {{\"id\": 1, \"description\": \"First sub-task\"}},\n",
        "                {{\"id\": 2, \"description\": \"Second sub-task\"}},\n",
        "                {{\"id\": 3, \"description\": \"Third sub-task\"}}\n",
        "            ]\n",
        "        }}\n",
        "\n",
        "        Provide ONLY the JSON, no other text.\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=planning_prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        try:\n",
        "            # Extract JSON from response\n",
        "            response_text = response.message.content.strip()\n",
        "            # Remove markdown code blocks if present\n",
        "            if response_text.startswith(\"```\"):\n",
        "                response_text = response_text.split(\"```\")[1]\n",
        "                if response_text.startswith(\"json\"):\n",
        "                    response_text = response_text[4:]\n",
        "\n",
        "            plan = json.loads(response_text)\n",
        "            subtasks = plan[\"subtasks\"]\n",
        "\n",
        "            print(\"\\nðŸ“‹ Orchestrator's Plan:\")\n",
        "            for subtask in subtasks:\n",
        "                print(f\"  {subtask['id']}. {subtask['description']}\")\n",
        "\n",
        "            return subtasks\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing plan: {e}\")\n",
        "            # Fallback to manual subtasks\n",
        "            return [\n",
        "                {\"id\": 1, \"description\": f\"Research and outline for: {task}\"},\n",
        "                {\"id\": 2, \"description\": f\"Write main content for: {task}\"},\n",
        "                {\"id\": 3, \"description\": f\"Review and finalize: {task}\"}\n",
        "            ]\n",
        "\n",
        "    def worker_execute(self, subtask: Dict[str, str]) -> str:\n",
        "        \"\"\"\n",
        "        Worker Agent: Executes a specific sub-task.\n",
        "        \"\"\"\n",
        "        subtask_id = subtask['id']\n",
        "        description = subtask['description']\n",
        "\n",
        "        print(f\"\\nðŸ‘· Worker Agent {subtask_id}: Executing sub-task...\")\n",
        "\n",
        "        execution_prompt = f\"\"\"You are a worker agent. Complete this specific sub-task concisely (2-3 sentences):\n",
        "\n",
        "        Sub-task: {description}\n",
        "\n",
        "        Provide your output:\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=execution_prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        result = response.message.content\n",
        "        print(f\"âœ… Worker {subtask_id} completed\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def orchestrator_aggregate(self, task: str, results: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Orchestrator Agent: Aggregates worker results into final output.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸŽ¯ Orchestrator Agent: Aggregating results...\")\n",
        "\n",
        "        combined_work = \"\\n\\n\".join([f\"Part {i+1}:\\n{result}\" for i, result in enumerate(results)])\n",
        "\n",
        "        aggregation_prompt = f\"\"\"You are the orchestrator. Combine these worker outputs into a cohesive final result for the task: {task}\n",
        "\n",
        "        Worker Outputs:\n",
        "        {combined_work}\n",
        "\n",
        "        Create a unified, polished final output:\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=aggregation_prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        return response.message.content\n",
        "\n",
        "    def run_demo(self, task: str):\n",
        "        \"\"\"Demonstrate the orchestrator-worker pattern.\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ðŸ‘” ORCHESTRATOR-WORKER PATTERN EXAMPLE\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nðŸ“Œ Main Task: {task}\")\n",
        "\n",
        "        # Step 1: Orchestrator creates plan\n",
        "        subtasks = self.orchestrator_plan(task)\n",
        "\n",
        "        # Step 2: Workers execute sub-tasks\n",
        "        results = []\n",
        "        for subtask in subtasks:\n",
        "            result = self.worker_execute(subtask)\n",
        "            results.append(result)\n",
        "\n",
        "        # Step 3: Orchestrator aggregates results\n",
        "        final_output = self.orchestrator_aggregate(task, results)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŽ‰ FINAL OUTPUT:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\n{final_output}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… Orchestrator-Worker Demo Complete!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# Run the example\n",
        "orchestrator_example = OrchestratorWorkerExample(llm)\n",
        "final_result = orchestrator_example.run_demo(\"Write a short blog post about the benefits of meditation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98908d5b",
      "metadata": {
        "id": "98908d5b"
      },
      "source": [
        "---\n",
        "\n",
        "### Pattern E: Evaluator-Optimizer\n",
        "\n",
        "**Concept**: Implement a feedback loop where one agent generates solutions and another evaluates and provides feedback for improvement.\n",
        "\n",
        "**Use Cases**:\n",
        "- Code review and refinement\n",
        "- Content quality improvement\n",
        "- Iterative problem-solving\n",
        "\n",
        "**Implementation**: A generator creates code, an evaluator critiques it, and the generator improves based on feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d2db414b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2db414b",
        "outputId": "2ba3683c-6f4f-43c9-dc88-1598adce9f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ” EVALUATOR-OPTIMIZER PATTERN EXAMPLE\n",
            "============================================================\n",
            "\n",
            "ðŸ“Œ Task: Write a Python function that checks if a number is prime\n",
            "\n",
            "============================================================\n",
            "ðŸ”„ ITERATION 1\n",
            "============================================================\n",
            "\n",
            "âœï¸ Generator Agent: Creating initial solution...\n",
            "\n",
            "ðŸ“„ Generated Solution:\n",
            "Here's a concise Python function to check if a number is prime. It first handles edge cases (numbers less than 2), then checks divisibility from 2 up to the square root of the number for efficiency:\n",
            "\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n < 2:\n",
            "        return False\n",
            "    for i in range(2, int(n**0.5) + 1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n",
            "\n",
            "ðŸ” Evaluator Agent: Analyzing solution...\n",
            "\n",
            "ðŸ“Š Evaluation:\n",
            "Score: 9  \n",
            "Acceptable: yes  \n",
            "Feedback: The solution is correct, efficient, and concise. For further improvement, consider adding a docstring to explain the function's purpose and inputs, and possibly handle non-integer inputs gracefully or document that the input should be an integer. Adding type hints could also enhance readability.\n",
            "\n",
            "âœ… Solution accepted!\n",
            "\n",
            "============================================================\n",
            "ðŸŽ‰ FINAL SOLUTION:\n",
            "============================================================\n",
            "\n",
            "Here's a concise Python function to check if a number is prime. It first handles edge cases (numbers less than 2), then checks divisibility from 2 up to the square root of the number for efficiency:\n",
            "\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n < 2:\n",
            "        return False\n",
            "    for i in range(2, int(n**0.5) + 1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n",
            "\n",
            "============================================================\n",
            "âœ… Evaluator-Optimizer Demo Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "class EvaluatorOptimizerExample:\n",
        "    \"\"\"Demonstrates evaluator-optimizer pattern with feedback loops.\"\"\"\n",
        "\n",
        "    def __init__(self, llm, max_iterations: int = 2):\n",
        "        self.llm = llm\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "    def generator_agent(self, task: str, feedback: str = None) -> str:\n",
        "        \"\"\"\n",
        "        Generator Agent: Creates a solution (with optional feedback incorporation).\n",
        "        \"\"\"\n",
        "        if feedback:\n",
        "            print(\"\\nðŸ”§ Generator Agent: Improving solution based on feedback...\")\n",
        "            prompt = f\"\"\"You previously worked on this task: {task}\n",
        "\n",
        "            You received this feedback:\n",
        "            {feedback}\n",
        "\n",
        "            Please create an improved version that addresses the feedback.\"\"\"\n",
        "        else:\n",
        "            print(\"\\nâœï¸ Generator Agent: Creating initial solution...\")\n",
        "            prompt = f\"\"\"Create a solution for this task: {task}\n",
        "\n",
        "            Provide a concise solution (3-4 sentences).\"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        solution = response.message.content\n",
        "        print(f\"\\nðŸ“„ Generated Solution:\\n{solution}\")\n",
        "\n",
        "        return solution\n",
        "\n",
        "    def evaluator_agent(self, task: str, solution: str) -> Dict[str, any]:\n",
        "        \"\"\"\n",
        "        Evaluator Agent: Critiques the solution and provides feedback.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸ” Evaluator Agent: Analyzing solution...\")\n",
        "\n",
        "        evaluation_prompt = f\"\"\"You are a critical evaluator. Assess this solution:\n",
        "\n",
        "        Task: {task}\n",
        "\n",
        "        Solution:\n",
        "        {solution}\n",
        "\n",
        "        Provide:\n",
        "        1. A quality score (1-10)\n",
        "        2. Specific feedback for improvement (if score < 9)\n",
        "        3. Whether it's acceptable (yes/no)\n",
        "\n",
        "        Format:\n",
        "        Score: [number]\n",
        "        Acceptable: [yes/no]\n",
        "        Feedback: [your feedback or 'None' if perfect]\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [ChatMessage(role=\"user\", content=evaluation_prompt)]\n",
        "        response = self.llm.chat(messages)\n",
        "\n",
        "        evaluation_text = response.message.content\n",
        "        print(f\"\\nðŸ“Š Evaluation:\\n{evaluation_text}\")\n",
        "\n",
        "        # Parse evaluation (simple heuristic)\n",
        "        lines = evaluation_text.lower().split('\\n')\n",
        "\n",
        "        acceptable = any('acceptable: yes' in line for line in lines)\n",
        "\n",
        "        # Extract feedback\n",
        "        feedback_lines = []\n",
        "        capture = False\n",
        "        for line in evaluation_text.split('\\n'):\n",
        "            if 'feedback:' in line.lower():\n",
        "                capture = True\n",
        "                feedback_lines.append(line.split(':', 1)[1].strip() if ':' in line else '')\n",
        "            elif capture:\n",
        "                feedback_lines.append(line)\n",
        "\n",
        "        feedback = ' '.join(feedback_lines).strip()\n",
        "        if not feedback or 'none' in feedback.lower():\n",
        "            feedback = None\n",
        "\n",
        "        return {\n",
        "            'acceptable': acceptable,\n",
        "            'feedback': feedback,\n",
        "            'evaluation_text': evaluation_text\n",
        "        }\n",
        "\n",
        "    def run_feedback_loop(self, task: str):\n",
        "        \"\"\"\n",
        "        Run the evaluator-optimizer feedback loop.\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ðŸ” EVALUATOR-OPTIMIZER PATTERN EXAMPLE\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nðŸ“Œ Task: {task}\")\n",
        "\n",
        "        solution = None\n",
        "        feedback = None\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"ðŸ”„ ITERATION {iteration + 1}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Generate (or improve) solution\n",
        "            solution = self.generator_agent(task, feedback)\n",
        "\n",
        "            # Evaluate solution\n",
        "            evaluation = self.evaluator_agent(task, solution)\n",
        "\n",
        "            # Check if acceptable\n",
        "            if evaluation['acceptable']:\n",
        "                print(\"\\nâœ… Solution accepted!\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"\\nâš ï¸ Solution needs improvement\")\n",
        "                feedback = evaluation['feedback']\n",
        "                if iteration < self.max_iterations - 1:\n",
        "                    print(f\"\\nðŸ“ Feedback for next iteration: {feedback}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸŽ‰ FINAL SOLUTION:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\n{solution}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… Evaluator-Optimizer Demo Complete!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return solution\n",
        "\n",
        "# Run the example\n",
        "evaluator_example = EvaluatorOptimizerExample(llm, max_iterations=2)\n",
        "final_solution = evaluator_example.run_feedback_loop(\n",
        "    \"Write a Python function that checks if a number is prime\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4669b996",
      "metadata": {
        "id": "4669b996"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Conclusion & Pattern Selection Guide\n",
        "\n",
        "### When to Use Each Pattern\n",
        "\n",
        "| Pattern | Best For | Key Benefit | Complexity |\n",
        "|---------|----------|-------------|------------|\n",
        "| **Prompt Chaining** | Sequential pipelines, data transformation | Simple, predictable flow | â­ Low |\n",
        "| **Routing** | Multi-domain systems, request classification | Specialization, efficiency | â­â­ Medium |\n",
        "| **Parallelization** | Independent tasks, time-sensitive operations | Speed, diverse perspectives | â­â­ Medium |\n",
        "| **Orchestrator-Worker** | Complex projects, hierarchical tasks | Organization, scalability | â­â­â­ High |\n",
        "| **Evaluator-Optimizer** | Quality-critical outputs, iterative refinement | Quality, self-improvement | â­â­â­ High |\n",
        "\n",
        "### Pattern Combinations\n",
        "\n",
        "These patterns can be **combined** for more sophisticated systems:\n",
        "\n",
        "- **Orchestrator + Routing**: Manager delegates to specialized workers\n",
        "- **Parallelization + Evaluator**: Multiple solutions generated, best one selected\n",
        "- **Chaining + Optimizer**: Each step in chain has evaluation/refinement\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Start Simple**: Begin with prompt chaining, add complexity as needed\n",
        "2. **Monitor Costs**: Even with free models, be mindful of rate limits and quotas\n",
        "3. **Error Handling**: Implement retries and fallbacks for production systems\n",
        "4. **Logging**: Track agent decisions and outputs for debugging\n",
        "5. **Human in the Loop**: Add checkpoints for critical decisions\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore LlamaIndex's built-in agent frameworks\n",
        "- Add RAG (Retrieval-Augmented Generation) to your agents\n",
        "- Implement tool use with agents\n",
        "- Build custom workflows with `llama_index.core.workflow`\n",
        "- Try other OpenRouter models as you scale\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
        "- [LlamaIndex Agent Guide](https://docs.llamaindex.ai/en/stable/understanding/agent/)\n",
        "---\n",
        "\n",
        "**Happy Building! ðŸš€**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}