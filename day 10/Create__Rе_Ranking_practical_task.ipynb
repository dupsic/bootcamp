{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Encoder Re-Ranking**"
      ],
      "metadata": {
        "id": "OG4wvjYyMCkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install sentence-transformers python library  and import 'CrossEncoder' from this library."
      ],
      "metadata": {
        "id": "xI1_9dt4MS5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# place your code here\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3bSsJe0xL-PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# place your code here"
      ],
      "metadata": {
        "id": "ePIiSGypRk7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From typing  import List, Tuple"
      ],
      "metadata": {
        "id": "FuT_qlkhRELf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# place your code here"
      ],
      "metadata": {
        "id": "p-eXZD6HSa9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained cross-encoder model 'cross-encoder/ms-marco-MiniLM-L-6-v2' and save the model object to the 'model' variable. To load, use CrossEncoder() with two arguments: the first is the model name as string and the second argument is 'max_length=512'"
      ],
      "metadata": {
        "id": "RuFp9X8fS1-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# place your code here"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z7mFAH7_VamV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use function:"
      ],
      "metadata": {
        "id": "HpMAY4elYS7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_documents(\n",
        "    query: str,  documents: List[str],  top_k: int = 10\n",
        ") -> List[Tuple[int, str, float]]:\n",
        "    \"\"\"\n",
        "    Re-rank documents based on relevance to query.\n",
        "    Args:\n",
        "        query: The search query\n",
        "        documents: List of document texts from initial retrieval\n",
        "        top_k: Number of top results to return\n",
        "    Returns:\n",
        "        List of (original_index, document, score) tuples, sorted by relevance\n",
        "    \"\"\"\n",
        "    # Create query-document pairs for the cross-encoder\n",
        "    # Each pair will be scored independently\n",
        "    pairs = [[query, doc] for doc in documents]\n",
        "\n",
        "    # Get relevance scores for all pairs\n",
        "    # The model outputs a single score per pair\n",
        "    scores = model.predict(pairs)\n",
        "\n",
        "    # Combine with original indices and sort by score descending\n",
        "    scored_docs = [\n",
        "        (idx, doc, float(score))\n",
        "        for idx, (doc, score) in enumerate(zip(documents, scores))\n",
        "    ]\n",
        "    scored_docs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    return scored_docs[:top_k]"
      ],
      "metadata": {
        "id": "wSED3rWbYEAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulate retrieval results: use the following data."
      ],
      "metadata": {
        "id": "AkwSr7FyZVVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = [\n",
        "        \"Kubernetes pod monitoring requires metrics collection from the kubelet.\",\n",
        "        \"Docker containers can be monitored using cAdvisor metrics.\",\n",
        "        \"To check pod status, use kubectl get pods command.\",\n",
        "        \"Prometheus is commonly used for Kubernetes monitoring.\",\n",
        "        \"Pod resource limits should be set in the deployment spec.\",\n",
        "        \"OneUptime provides real-time Kubernetes monitoring dashboards.\",\n",
        "        \"Container orchestration platforms need observability solutions.\",\n",
        "        \"The kubectl logs command shows container output.\",\n",
        "    ]\n",
        "query = \"How do I monitor Kubernetes pods?\""
      ],
      "metadata": {
        "id": "o9a0kiHkeWRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select a value for the k parameter and apply the \"rerank_documents()\" function to the data.\n",
        "Save the returned object to the results variabl."
      ],
      "metadata": {
        "id": "IqoLs-IovCFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# place your code here."
      ],
      "metadata": {
        "id": "0i7zeLO2a1Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the 'for idx, doc, score in results'    loop, print the values ​​of 'idx', 'doc' and 'score'"
      ],
      "metadata": {
        "id": "OOOy9YNatZKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Re-ranked results:\")\n",
        "# place your code here"
      ],
      "metadata": {
        "id": "PQq-EvBJbY9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}